{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play with src/star.py module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from src/multi_attention_forward.py\n",
    "from torch.nn.functional import softmax, dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. get_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise(shape, noise_type):\n",
    "    if noise_type == \"gaussian\":\n",
    "        return torch.randn(shape) #.cuda()\n",
    "    elif noise_type == \"uniform\":\n",
    "        return torch.rand(*shape).sub_(0.5).mul_(2.0) #.cuda()\n",
    "    raise ValueError('Unrecognized noise type \"%s\"' % noise_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8112, -0.0395, -0.0106,  0.0021,  0.9918,  0.6169,  0.0622,  0.4368],\n",
       "        [ 0.1090, -0.8407,  0.4117, -0.8674, -0.6100, -0.2603, -0.5763, -0.6788],\n",
       "        [-0.6423, -0.4598, -0.8704,  0.7628,  0.0550,  0.6299, -0.5815,  0.7300]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_noise((3,8), \"uniform\") #.max().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. get_subsequent_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subsequent_mask(seq):\n",
    "    ''' For masking out the subsequent info. '''\n",
    "    sz_b, len_s = seq.size()\n",
    "    subsequent_mask = (1 - torch.triu(\n",
    "        torch.ones((1, len_s, len_s), device=seq.device), diagonal=1)).bool()\n",
    "    return subsequent_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False, False, False, False, False, False],\n",
       "         [ True,  True, False, False, False, False, False, False],\n",
       "         [ True,  True,  True, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = get_noise((1,8), \"uniform\")\n",
    "get_subsequent_mask(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear and multi_head_attention_forward from src/multi_attention_forward.py\n",
    "\n",
    "This cell contains the forward pass of the Multi-head Attention layer. This classs if very general and can deal with a lot of paraemeters and expections, but at the end you will not touch it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(input, weight, bias=None):\n",
    "    # type: (Tensor, Tensor, Optional[Tensor]) -> Tensor\n",
    "    r\"\"\"\n",
    "    Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.\n",
    "    Shape:\n",
    "        - Input: :math:`(N, *, in\\_features)` where `*` means any number of\n",
    "          additional dimensions\n",
    "        - Weight: :math:`(out\\_features, in\\_features)`\n",
    "        - Bias: :math:`(out\\_features)`\n",
    "        - Output: :math:`(N, *, out\\_features)`\n",
    "    \"\"\"\n",
    "    if input.dim() == 2 and bias is not None:\n",
    "        # fused op is marginally faster\n",
    "        ret = torch.addmm(bias, input, weight.t())\n",
    "    else:\n",
    "        output = input.matmul(weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias\n",
    "        ret = output\n",
    "    return ret\n",
    "\n",
    "\n",
    "def multi_head_attention_forward(query,                           # type: Tensor\n",
    "                                 key,                             # type: Tensor\n",
    "                                 value,                           # type: Tensor\n",
    "                                 embed_dim_to_check,              # type: int\n",
    "                                 num_heads,                       # type: int\n",
    "                                 in_proj_weight,                  # type: Tensor\n",
    "                                 in_proj_bias,                    # type: Tensor\n",
    "                                 bias_k,                          # type: Optional[Tensor]\n",
    "                                 bias_v,                          # type: Optional[Tensor]\n",
    "                                 add_zero_attn,                   # type: bool\n",
    "                                 dropout_p,                       # type: float\n",
    "                                 out_proj_weight,                 # type: Tensor\n",
    "                                 out_proj_bias,                   # type: Tensor\n",
    "                                 training=True,                   # type: bool\n",
    "                                 key_padding_mask=None,           # type: Optional[Tensor]\n",
    "                                 need_weights=True,               # type: bool\n",
    "                                 attn_mask=None,                  # type: Optional[Tensor]\n",
    "                                 use_separate_proj_weight=False,  # type: bool\n",
    "                                 q_proj_weight=None,              # type: Optional[Tensor]\n",
    "                                 k_proj_weight=None,              # type: Optional[Tensor]\n",
    "                                 v_proj_weight=None,              # type: Optional[Tensor]\n",
    "                                 static_k=None,                   # type: Optional[Tensor]\n",
    "                                 static_v=None                    # type: Optional[Tensor]\n",
    "                                 ):\n",
    "    # type: (...) -> Tuple[Tensor, Optional[Tensor]]\n",
    "    r\"\"\"\n",
    "    Args:\n",
    "        query, key, value: map a query and a set of key-value pairs to an output.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        embed_dim_to_check: total dimension of the model.\n",
    "        num_heads: parallel attention heads.\n",
    "        in_proj_weight, in_proj_bias: input projection weight and bias.\n",
    "        bias_k, bias_v: bias of the key and value sequences to be added at dim=0.\n",
    "        add_zero_attn: add a new batch of zeros to the key and\n",
    "                       value sequences at dim=1.\n",
    "        dropout_p: probability of an element to be zeroed.\n",
    "        out_proj_weight, out_proj_bias: the output projection weight and bias.\n",
    "        training: apply dropout if is ``True``.\n",
    "        key_padding_mask: if provided, specified padding elements in the key will\n",
    "            be ignored by the attention. This is an binary mask. When the value is True,\n",
    "            the corresponding value on the attention layer will be filled with -inf.\n",
    "        need_weights: output attn_output_weights.\n",
    "        attn_mask: mask that prevents attention to certain positions. This is an additive mask\n",
    "            (i.e. the values will be added to the attention layer).\n",
    "        use_separate_proj_weight: the function accept the proj. weights for query, key,\n",
    "            and value in different forms. If false, in_proj_weight will be used, which is\n",
    "            a combination of q_proj_weight, k_proj_weight, v_proj_weight.\n",
    "        q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.\n",
    "        static_k, static_v: static key and value used for attention operators.\n",
    "    Shape:\n",
    "        Inputs:\n",
    "        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key_padding_mask: :math:`(N, S)`, ByteTensor, where N is the batch size, S is the source sequence length.\n",
    "        - attn_mask: :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n",
    "        - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n",
    "          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n",
    "        - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n",
    "          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n",
    "        Outputs:\n",
    "        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
    "          E is the embedding dimension.\n",
    "        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n",
    "          L is the target sequence length, S is the source sequence length.\n",
    "    \"\"\"\n",
    "\n",
    "    tgt_len, bsz, embed_dim = query.size()\n",
    "    assert embed_dim == embed_dim_to_check\n",
    "    assert key.size() == value.size()\n",
    "\n",
    "    head_dim = embed_dim // num_heads\n",
    "    assert head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "    scaling = float(head_dim) ** -0.5\n",
    "\n",
    "    if not use_separate_proj_weight:\n",
    "        if torch.equal(query, key) and torch.equal(key, value):\n",
    "            # self-attention\n",
    "            q, k, v = linear(query, in_proj_weight, in_proj_bias).chunk(3, dim=-1)\n",
    "\n",
    "        elif torch.equal(key, value):\n",
    "            # encoder-decoder attention\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = 0\n",
    "            _end = embed_dim\n",
    "            _w = in_proj_weight[_start:_end, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:_end]\n",
    "            q = linear(query, _w, _b)\n",
    "\n",
    "            if key is None:\n",
    "                assert value is None\n",
    "                k = None\n",
    "                v = None\n",
    "            else:\n",
    "\n",
    "                # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "                _b = in_proj_bias\n",
    "                _start = embed_dim\n",
    "                _end = None\n",
    "                _w = in_proj_weight[_start:, :]\n",
    "                if _b is not None:\n",
    "                    _b = _b[_start:]\n",
    "                k, v = linear(key, _w, _b).chunk(2, dim=-1)\n",
    "\n",
    "        else:\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = 0\n",
    "            _end = embed_dim\n",
    "            _w = in_proj_weight[_start:_end, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:_end]\n",
    "            q = linear(query, _w, _b)\n",
    "\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = embed_dim\n",
    "            _end = embed_dim * 2\n",
    "            _w = in_proj_weight[_start:_end, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:_end]\n",
    "            k = linear(key, _w, _b)\n",
    "\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = embed_dim * 2\n",
    "            _end = None\n",
    "            _w = in_proj_weight[_start:, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:]\n",
    "            v = linear(value, _w, _b)\n",
    "    else:\n",
    "        q_proj_weight_non_opt = torch.jit._unwrap_optional(q_proj_weight)\n",
    "        len1, len2 = q_proj_weight_non_opt.size()\n",
    "        assert len1 == embed_dim and len2 == query.size(-1)\n",
    "\n",
    "        k_proj_weight_non_opt = torch.jit._unwrap_optional(k_proj_weight)\n",
    "        len1, len2 = k_proj_weight_non_opt.size()\n",
    "        assert len1 == embed_dim and len2 == key.size(-1)\n",
    "\n",
    "        v_proj_weight_non_opt = torch.jit._unwrap_optional(v_proj_weight)\n",
    "        len1, len2 = v_proj_weight_non_opt.size()\n",
    "        assert len1 == embed_dim and len2 == value.size(-1)\n",
    "\n",
    "        if in_proj_bias is not None:\n",
    "            q = linear(query, q_proj_weight_non_opt, in_proj_bias[0:embed_dim])\n",
    "            k = linear(key, k_proj_weight_non_opt, in_proj_bias[embed_dim:(embed_dim * 2)])\n",
    "            v = linear(value, v_proj_weight_non_opt, in_proj_bias[(embed_dim * 2):])\n",
    "        else:\n",
    "            q = linear(query, q_proj_weight_non_opt, in_proj_bias)\n",
    "            k = linear(key, k_proj_weight_non_opt, in_proj_bias)\n",
    "            v = linear(value, v_proj_weight_non_opt, in_proj_bias)\n",
    "    q = q * scaling\n",
    "\n",
    "    if bias_k is not None and bias_v is not None:\n",
    "        if static_k is None and static_v is None:\n",
    "            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\n",
    "            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])\n",
    "            if attn_mask is not None:\n",
    "                attn_mask = torch.cat([attn_mask,\n",
    "                                      torch.zeros((attn_mask.size(0), 1),\n",
    "                                                  dtype=attn_mask.dtype,\n",
    "                                                  device=attn_mask.device)], dim=1)\n",
    "            if key_padding_mask is not None:\n",
    "                key_padding_mask = torch.cat(\n",
    "                    [key_padding_mask, torch.zeros((key_padding_mask.size(0), 1),\n",
    "                                                   dtype=key_padding_mask.dtype,\n",
    "                                                   device=key_padding_mask.device)], dim=1)\n",
    "        else:\n",
    "            assert static_k is None, \"bias cannot be added to static key.\"\n",
    "            assert static_v is None, \"bias cannot be added to static value.\"\n",
    "    else:\n",
    "        assert bias_k is None\n",
    "        assert bias_v is None\n",
    "\n",
    "    q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    if k is not None:\n",
    "        k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    if v is not None:\n",
    "        v = v.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    if static_k is not None:\n",
    "        assert static_k.size(0) == bsz * num_heads\n",
    "        assert static_k.size(2) == head_dim\n",
    "        k = static_k\n",
    "\n",
    "    if static_v is not None:\n",
    "        assert static_v.size(0) == bsz * num_heads\n",
    "        assert static_v.size(2) == head_dim\n",
    "        v = static_v\n",
    "\n",
    "    src_len = k.size(1)\n",
    "\n",
    "    if key_padding_mask is not None:\n",
    "        assert key_padding_mask.size(0) == bsz\n",
    "        assert key_padding_mask.size(1) == src_len\n",
    "\n",
    "    if add_zero_attn:\n",
    "        src_len += 1\n",
    "        k = torch.cat([k, torch.zeros((k.size(0), 1) + k.size()[2:], dtype=k.dtype, device=k.device)], dim=1)\n",
    "        v = torch.cat([v, torch.zeros((v.size(0), 1) + v.size()[2:], dtype=v.dtype, device=v.device)], dim=1)\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = torch.cat([attn_mask, torch.zeros((attn_mask.size(0), 1),\n",
    "                                                          dtype=attn_mask.dtype,\n",
    "                                                          device=attn_mask.device)], dim=1)\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = torch.cat(\n",
    "                [key_padding_mask, torch.zeros((key_padding_mask.size(0), 1),\n",
    "                                               dtype=key_padding_mask.dtype,\n",
    "                                               device=key_padding_mask.device)], dim=1)\n",
    "\n",
    "    attn_output_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "    assert list(attn_output_weights.size()) == [bsz * num_heads, tgt_len, src_len]\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        attn_mask = attn_mask.unsqueeze(0)\n",
    "        attn_output_weights += attn_mask\n",
    "\n",
    "    if key_padding_mask is not None:\n",
    "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
    "        attn_output_weights = attn_output_weights.masked_fill(\n",
    "            key_padding_mask.unsqueeze(1).unsqueeze(2),\n",
    "            float('-inf'),\n",
    "        )\n",
    "        attn_output_weights = attn_output_weights.view(bsz * num_heads, tgt_len, src_len)\n",
    "\n",
    "    attn_output_weights = softmax(\n",
    "        attn_output_weights, dim=-1)\n",
    "\n",
    "    attn_output_weights = dropout(attn_output_weights, p=dropout_p, training=training)\n",
    "\n",
    "    attn_output = torch.bmm(attn_output_weights, v)\n",
    "    # import pdb; pdb.set_trace()\n",
    "    assert list(attn_output.size()) == [bsz * num_heads, tgt_len, head_dim]\n",
    "    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
    "    attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
    "    if need_weights:\n",
    "        # average attention weights over heads\n",
    "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
    "        return attn_output, attn_output_weights.sum(dim=1) / num_heads\n",
    "    else:\n",
    "        return attn_output, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MultiheadAttention\n",
    "\n",
    "Perform the multi-head self-attention layer.\n",
    "During the forward pass, it takes 3 inputs: (query, key, value). These could be (src, src, src) in case of the source sentence self-attention, or (target, src, src) in case of the encoder-decoder self-attention.\n",
    "\n",
    "This classs if very general and can deal with a lot of paraemeters and expections, but at the end you will not touch it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    r\"\"\"Allows the model to jointly attend to information\n",
    "    from different representation subspaces.\n",
    "    See reference: Attention Is All You Need\n",
    "    .. math::\n",
    "        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n",
    "        \\text{where} head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "    Args:\n",
    "        embed_dim: total dimension of the model.\n",
    "        num_heads: parallel attention heads.\n",
    "        dropout: a Dropout layer on attn_output_weights. Default: 0.0.\n",
    "        bias: add bias as module parameter. Default: True.\n",
    "        add_bias_kv: add bias to the key and value sequences at dim=0.\n",
    "        add_zero_attn: add a new batch of zeros to the key and\n",
    "                       value sequences at dim=1.\n",
    "        kdim: total number of features in key. Default: None.\n",
    "        vdim: total number of features in key. Default: None.\n",
    "        Note: if kdim and vdim are None, they will be set to embed_dim such that\n",
    "        query, key, and value have the same number of features.\n",
    "    Examples::\n",
    "        >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
    "    \"\"\"\n",
    "    __constants__ = ['q_proj_weight', 'k_proj_weight', 'v_proj_weight', 'in_proj_weight']\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False, kdim=None,\n",
    "                 vdim=None):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.kdim = kdim if kdim is not None else embed_dim\n",
    "        self.vdim = vdim if vdim is not None else embed_dim\n",
    "        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        if self._qkv_same_embed_dim is False:\n",
    "            self.q_proj_weight = nn.Parameter(torch.Tensor(embed_dim, embed_dim))\n",
    "            self.k_proj_weight = nn.Parameter(torch.Tensor(embed_dim, self.kdim))\n",
    "            self.v_proj_weight = nn.Parameter(torch.Tensor(embed_dim, self.vdim))\n",
    "            self.register_parameter('in_proj_weight', None)\n",
    "        else:\n",
    "            self.in_proj_weight = nn.Parameter(torch.empty(3 * embed_dim, embed_dim))\n",
    "            self.register_parameter('q_proj_weight', None)\n",
    "            self.register_parameter('k_proj_weight', None)\n",
    "            self.register_parameter('v_proj_weight', None)\n",
    "\n",
    "        if bias:\n",
    "            self.in_proj_bias = nn.Parameter(torch.empty(3 * embed_dim))\n",
    "        else:\n",
    "            self.register_parameter('in_proj_bias', None)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "        if add_bias_kv:\n",
    "            self.bias_k = nn.Parameter(torch.empty(1, 1, embed_dim))\n",
    "            self.bias_v = nn.Parameter(torch.empty(1, 1, embed_dim))\n",
    "        else:\n",
    "            self.bias_k = self.bias_v = None\n",
    "\n",
    "        self.add_zero_attn = add_zero_attn\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        if self._qkv_same_embed_dim:\n",
    "            nn.init.xavier_uniform_(self.in_proj_weight)\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(self.q_proj_weight)\n",
    "            nn.init.xavier_uniform_(self.k_proj_weight)\n",
    "            nn.init.xavier_uniform_(self.v_proj_weight)\n",
    "\n",
    "        if self.in_proj_bias is not None:\n",
    "            nn.init.constant_(self.in_proj_bias, 0.)\n",
    "            nn.init.constant_(self.out_proj.bias, 0.)\n",
    "        if self.bias_k is not None:\n",
    "            nn.init.xavier_normal_(self.bias_k)\n",
    "        if self.bias_v is not None:\n",
    "            nn.init.xavier_normal_(self.bias_v)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        # Support loading old MultiheadAttention checkpoints generated by v1.1.0\n",
    "        if '_qkv_same_embed_dim' not in state:\n",
    "            state['_qkv_same_embed_dim'] = True\n",
    "\n",
    "        super(MultiheadAttention, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask=None,\n",
    "                need_weights=True, attn_mask=None):\n",
    "        # type: (Tensor, Tensor, Tensor, Optional[Tensor], bool, Optional[Tensor]) -> Tuple[Tensor, Optional[Tensor]]\n",
    "        r\"\"\"\n",
    "    Args:\n",
    "        query, key, value: map a query and a set of key-value pairs to an output.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        key_padding_mask: if provided, specified padding elements in the key will\n",
    "            be ignored by the attention. This is an binary mask. When the value is True,\n",
    "            the corresponding value on the attention layer will be filled with -inf.\n",
    "        need_weights: output attn_output_weights.\n",
    "        attn_mask: mask that prevents attention to certain positions. This is an additive mask\n",
    "            (i.e. the values will be added to the attention layer).\n",
    "    Shape:\n",
    "        - Inputs:\n",
    "        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key_padding_mask: :math:`(N, S)`, ByteTensor, where N is the batch size, S is the source sequence length.\n",
    "        - attn_mask: :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n",
    "        - Outputs:\n",
    "        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
    "          E is the embedding dimension.\n",
    "        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n",
    "          L is the target sequence length, S is the source sequence length.\n",
    "        \"\"\"\n",
    "        if not self._qkv_same_embed_dim:\n",
    "            return multi_head_attention_forward(\n",
    "                query, key, value, self.embed_dim, self.num_heads,\n",
    "                self.in_proj_weight, self.in_proj_bias,\n",
    "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                training=self.training,\n",
    "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "                attn_mask=attn_mask, use_separate_proj_weight=True,\n",
    "                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n",
    "                v_proj_weight=self.v_proj_weight)\n",
    "        else:\n",
    "            return multi_head_attention_forward(\n",
    "                query, key, value, self.embed_dim, self.num_heads,\n",
    "                self.in_proj_weight, self.in_proj_bias,\n",
    "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                training=self.training,\n",
    "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "                attn_mask=attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of attn_output: torch.Size([12, 100, 512])\n",
      "Size of attn_output_weights: torch.Size([100, 12, 10])\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "batch_size = 100\n",
    "target_sequence_length = 12\n",
    "source_sequence_length = 10\n",
    "\n",
    "# layer init\n",
    "multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "\n",
    "query = torch.randn(target_sequence_length, batch_size, embed_dim)\n",
    "key = torch.randn(source_sequence_length, batch_size, embed_dim)\n",
    "value = key.clone()\n",
    "\n",
    "# forward pass9\n",
    "attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
    "\n",
    "print(\"Size of attn_output:\", attn_output.size())\n",
    "print(\"Size of attn_output_weights:\", attn_output_weights.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transformer Encoder Layer\n",
    "\n",
    "Perform Self-attention and Fully Connected, with some dropouts and skip connections in the way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0, activation=\"relu\"):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "\n",
    "        Args:\n",
    "            src: the sequnce to the encoder layer (required).\n",
    "            src_mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        src2, attn = self.self_attn(src, src, src, attn_mask=src_mask,\n",
    "                                    key_padding_mask=src_key_padding_mask)\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "\n",
    "        if hasattr(self, \"activation\"):\n",
    "            src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        else:  # for backward compatibility\n",
    "            src2 = self.linear2(self.dropout(F.relu(self.linear1(src))))\n",
    "\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Transformer Encoder\n",
    "\n",
    "A stack of N Transformer encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    r\"\"\"TransformerEncoder is a stack of N encoder layers\n",
    "\n",
    "    Args:\n",
    "        encoder_layer: an instance of the TransformerEncoderLayer() class (required).\n",
    "        num_layers: the number of sub-encoder-layers in the encoder (required).\n",
    "        norm: the layer normalization component (optional).\n",
    "\n",
    "    Examples::\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        >>> transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "        >>> src = torch.rand(10, 32, 512)\n",
    "        >>> out = transformer_encoder(src)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = _get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, src, mask=None, src_key_padding_mask=None):\n",
    "        r\"\"\"Pass the input through the encoder layers in turn.\n",
    "\n",
    "        Args:\n",
    "            src: the sequnce to the encoder (required).\n",
    "            mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        output = src\n",
    "\n",
    "        atts = []\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            output, attn = self.layers[i](output, src_mask=mask,\n",
    "                                          src_key_padding_mask=src_key_padding_mask)\n",
    "            atts.append(attn)\n",
    "        if self.norm:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Transformer module\n",
    "\n",
    "Wrapper around the Trannsformer encoder module. Build the n_mask before encoding.\n",
    "There is no decoder, this model only outputs an encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.ninp = ninp\n",
    "\n",
    "    def forward(self, src, mask):\n",
    "        n_mask = mask + torch.eye(mask.shape[0], mask.shape[0]).cuda()\n",
    "        n_mask = n_mask.float().masked_fill(n_mask == 0., float(-1e20)).masked_fill(n_mask == 1., float(0.0))\n",
    "        output = self.transformer_encoder(src, mask=n_mask)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. The STAR model architecture\n",
    "\n",
    "Look at the paper. Implement the STAR model.\n",
    "Contains the external memory module, the temporal transformers and the spacial transformers.\n",
    "There are many helping methods for dealing with sequences and graphs of pedestrians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STAR(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, args, dropout_prob=0):\n",
    "        super(STAR, self).__init__()\n",
    "\n",
    "        # set parameters for network architecture\n",
    "        self.embedding_size = [32]\n",
    "        self.output_size = 2\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.args = args\n",
    "\n",
    "        self.temporal_encoder_layer = TransformerEncoderLayer(d_model=32, nhead=8)\n",
    "\n",
    "        emsize = 32  # embedding dimension\n",
    "        nhid = 2048  # the dimension of the feedforward network model in TransformerEncoder\n",
    "        nlayers = 2  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "        nhead = 8  # the number of heads in the multihead-attention models\n",
    "        dropout = 0.1  # the dropout value\n",
    "\n",
    "        self.spatial_encoder_1 = TransformerModel(emsize, nhead, nhid, nlayers, dropout)\n",
    "        self.spatial_encoder_2 = TransformerModel(emsize, nhead, nhid, nlayers, dropout)\n",
    "\n",
    "        self.temporal_encoder_1 = TransformerEncoder(self.temporal_encoder_layer, 1)\n",
    "        self.temporal_encoder_2 = TransformerEncoder(self.temporal_encoder_layer, 1)\n",
    "\n",
    "        # Linear layer to map input to embedding\n",
    "        self.input_embedding_layer_temporal = nn.Linear(2, 32)\n",
    "        self.input_embedding_layer_spatial = nn.Linear(2, 32)\n",
    "\n",
    "        # Linear layer to output and fusion\n",
    "        self.output_layer = nn.Linear(48, 2)\n",
    "        self.fusion_layer = nn.Linear(64, 32)\n",
    "\n",
    "        # ReLU and dropout init\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout_in = nn.Dropout(self.dropout_prob)\n",
    "        self.dropout_in2 = nn.Dropout(self.dropout_prob)\n",
    "\n",
    "    def get_st_ed(self, batch_num):\n",
    "        \"\"\"\n",
    "\n",
    "        :param batch_num: contains number of pedestrians in different scenes for a batch\n",
    "        :type batch_num: list\n",
    "        :return: st_ed: list of tuple contains start index and end index of pedestrians in different scenes\n",
    "        :rtype: list\n",
    "        \"\"\"\n",
    "        cumsum = torch.cumsum(batch_num, dim=0)\n",
    "        st_ed = []\n",
    "        for idx in range(1, cumsum.shape[0]):\n",
    "            st_ed.append((int(cumsum[idx - 1]), int(cumsum[idx])))\n",
    "\n",
    "        st_ed.insert(0, (0, int(cumsum[0])))\n",
    "\n",
    "        return st_ed\n",
    "\n",
    "    def get_node_index(self, seq_list):\n",
    "        \"\"\"\n",
    "\n",
    "        :param seq_list: mask indicates whether pedestrain exists\n",
    "        :type seq_list: numpy array [F, N], F: number of frames. N: Number of pedestrians (a mask to indicate whether\n",
    "                                                                                            the pedestrian exists)\n",
    "        :return: All the pedestrians who exist from the beginning to current frame\n",
    "        :rtype: numpy array\n",
    "        \"\"\"\n",
    "        for idx, framenum in enumerate(seq_list):\n",
    "\n",
    "            if idx == 0:\n",
    "                node_indices = framenum > 0\n",
    "            else:\n",
    "                node_indices *= (framenum > 0)\n",
    "\n",
    "        return node_indices\n",
    "\n",
    "    def update_batch_pednum(self, batch_pednum, ped_list):\n",
    "        \"\"\"\n",
    "\n",
    "        :param batch_pednum: batch_num: contains number of pedestrians in different scenes for a batch\n",
    "        :type list\n",
    "        :param ped_list: mask indicates whether the pedestrian exists through the time window to current frame\n",
    "        :type tensor\n",
    "        :return: batch_pednum: contains number of pedestrians in different scenes for a batch after removing pedestrian who disappeared\n",
    "        :rtype: list\n",
    "        \"\"\"\n",
    "        updated_batch_pednum_ = copy.deepcopy(batch_pednum).cpu().numpy()\n",
    "        updated_batch_pednum = copy.deepcopy(batch_pednum)\n",
    "\n",
    "        cumsum = np.cumsum(updated_batch_pednum_)\n",
    "        new_ped = copy.deepcopy(ped_list).cpu().numpy()\n",
    "\n",
    "        for idx, num in enumerate(cumsum):\n",
    "            num = int(num)\n",
    "            if idx == 0:\n",
    "                updated_batch_pednum[idx] = len(np.where(new_ped[0:num] == 1)[0])\n",
    "            else:\n",
    "                updated_batch_pednum[idx] = len(np.where(new_ped[int(cumsum[idx - 1]):num] == 1)[0])\n",
    "\n",
    "        return updated_batch_pednum\n",
    "\n",
    "    def mean_normalize_abs_input(self, node_abs, st_ed):\n",
    "        \"\"\"\n",
    "\n",
    "        :param node_abs: Absolute coordinates of pedestrians\n",
    "        :type Tensor\n",
    "        :param st_ed: list of tuple indicates the indices of pedestrians belonging to the same scene\n",
    "        :type List of tupule\n",
    "        :return: node_abs: Normalized absolute coordinates of pedestrians\n",
    "        :rtype: Tensor\n",
    "        \"\"\"\n",
    "        node_abs = node_abs.permute(1, 0, 2)\n",
    "        for st, ed in st_ed:\n",
    "            mean_x = torch.mean(node_abs[st:ed, :, 0])\n",
    "            mean_y = torch.mean(node_abs[st:ed, :, 1])\n",
    "\n",
    "            node_abs[st:ed, :, 0] = (node_abs[st:ed, :, 0] - mean_x)\n",
    "            node_abs[st:ed, :, 1] = (node_abs[st:ed, :, 1] - mean_y)\n",
    "\n",
    "        return node_abs.permute(1, 0, 2)\n",
    "\n",
    "    def forward(self, inputs, iftest=False):\n",
    "\n",
    "        nodes_abs, nodes_norm, shift_value, seq_list, nei_lists, nei_num, batch_pednum = inputs\n",
    "        num_Ped = nodes_norm.shape[1]\n",
    "\n",
    "        outputs = torch.zeros(nodes_norm.shape[0], num_Ped, 2).cuda()\n",
    "        GM = torch.zeros(nodes_norm.shape[0], num_Ped, 32).cuda()\n",
    "\n",
    "        noise = get_noise((1, 16), 'gaussian')\n",
    "\n",
    "        for framenum in range(self.args.seq_length - 1):\n",
    "\n",
    "            if framenum >= self.args.obs_length and iftest:\n",
    "\n",
    "                node_index = self.get_node_index(seq_list[:self.args.obs_length])\n",
    "                updated_batch_pednum = self.update_batch_pednum(batch_pednum, node_index)\n",
    "                st_ed = self.get_st_ed(updated_batch_pednum)\n",
    "\n",
    "                nodes_current = outputs[self.args.obs_length - 1:framenum, node_index]\n",
    "                nodes_current = torch.cat((nodes_norm[:self.args.obs_length, node_index], nodes_current))\n",
    "                node_abs_base = nodes_abs[:self.args.obs_length, node_index]\n",
    "                node_abs_pred = shift_value[self.args.obs_length:framenum + 1, node_index] + outputs[\n",
    "                                                                                           self.args.obs_length - 1:framenum,\n",
    "                                                                                           node_index]\n",
    "                node_abs = torch.cat((node_abs_base, node_abs_pred), dim=0)\n",
    "                # We normalize the absolute coordinates using the mean value in the same scene\n",
    "                node_abs = self.mean_normalize_abs_input(node_abs, st_ed)\n",
    "\n",
    "            else:\n",
    "                node_index = self.get_node_index(seq_list[:framenum + 1])\n",
    "                nei_list = nei_lists[framenum, node_index, :]\n",
    "                nei_list = nei_list[:, node_index]\n",
    "                updated_batch_pednum = self.update_batch_pednum(batch_pednum, node_index)\n",
    "                st_ed = self.get_st_ed(updated_batch_pednum)\n",
    "                nodes_current = nodes_norm[:framenum + 1, node_index]\n",
    "                # We normalize the absolute coordinates using the mean value in the same scene\n",
    "                node_abs = self.mean_normalize_abs_input(nodes_abs[:framenum + 1, node_index], st_ed)\n",
    "\n",
    "            # Input Embedding\n",
    "            if framenum == 0:\n",
    "                temporal_input_embedded = self.dropout_in(self.relu(self.input_embedding_layer_temporal(nodes_current)))\n",
    "            else:\n",
    "                temporal_input_embedded = self.dropout_in(self.relu(self.input_embedding_layer_temporal(nodes_current)))\n",
    "                temporal_input_embedded[:framenum] = GM[:framenum, node_index]\n",
    "\n",
    "            spatial_input_embedded_ = self.dropout_in2(self.relu(self.input_embedding_layer_spatial(node_abs)))\n",
    "\n",
    "            spatial_input_embedded = self.spatial_encoder_1(spatial_input_embedded_[-1].unsqueeze(1), nei_list)\n",
    "\n",
    "            spatial_input_embedded = spatial_input_embedded.permute(1, 0, 2)[-1]\n",
    "            temporal_input_embedded_last = self.temporal_encoder_1(temporal_input_embedded)[-1]\n",
    "\n",
    "            temporal_input_embedded = temporal_input_embedded[:-1]\n",
    "\n",
    "            fusion_feat = torch.cat((temporal_input_embedded_last, spatial_input_embedded), dim=1)\n",
    "            fusion_feat = self.fusion_layer(fusion_feat)\n",
    "\n",
    "            spatial_input_embedded = self.spatial_encoder_2(fusion_feat.unsqueeze(1), nei_list)\n",
    "            spatial_input_embedded = spatial_input_embedded.permute(1, 0, 2)\n",
    "\n",
    "            temporal_input_embedded = torch.cat((temporal_input_embedded, spatial_input_embedded), dim=0)\n",
    "            temporal_input_embedded = self.temporal_encoder_2(temporal_input_embedded)[-1]\n",
    "\n",
    "            noise_to_cat = noise.repeat(temporal_input_embedded.shape[0], 1)\n",
    "            temporal_input_embedded_wnoise = torch.cat((temporal_input_embedded, noise_to_cat), dim=1)\n",
    "            outputs_current = self.output_layer(temporal_input_embedded_wnoise)\n",
    "            outputs[framenum, node_index] = outputs_current\n",
    "            GM[framenum, node_index] = temporal_input_embedded\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
