{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play with Trajectory_Dataloader class #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME_TO_NUM = {\n",
    "    'eth': 0,\n",
    "    'hotel': 1,\n",
    "    'zara1': 2,\n",
    "    'zara2': 3,\n",
    "    'univ': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "args = Object()\n",
    "args.dataset = 'eth5'\n",
    "args.test_set = 'zara1'\n",
    "args.save_dir = './output/' + args.test_set + \"/\"\n",
    "args.seq_length = 20\n",
    "args.batch_size = 8\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.makedirs(args.save_dir)\n",
    "\n",
    "self = Object()\n",
    "self.args = args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "if self.args.dataset == 'eth5':\n",
    "\n",
    "    self.data_dirs = ['../data/eth/univ', '../data/eth/hotel',\n",
    "                      '../data/ucy/zara/zara01', '../data/ucy/zara/zara02',\n",
    "                      '../data/ucy/univ/students001', '../data/ucy/univ/students003',\n",
    "                      '../data/ucy/univ/uni_examples', '../data/ucy/zara/zara03']\n",
    "\n",
    "    # Data directory where the pre-processed pickle file resides\n",
    "    self.data_dir = './data'\n",
    "    skip = [6, 10, 10, 10, 10, 10, 10, 10]\n",
    "\n",
    "    train_set = [i for i in range(len(self.data_dirs))]\n",
    "\n",
    "    assert args.test_set in DATASET_NAME_TO_NUM.keys(), 'Unsupported dataset {}'.format(args.test_set)\n",
    "\n",
    "    args.test_set = DATASET_NAME_TO_NUM[args.test_set]\n",
    "\n",
    "    if args.test_set == 4 or args.test_set == 5:\n",
    "        self.test_set = [4, 5]\n",
    "    else:\n",
    "        self.test_set = [self.args.test_set]\n",
    "\n",
    "    for x in self.test_set:\n",
    "        train_set.remove(x)\n",
    "\n",
    "    self.train_dir = [self.data_dirs[x] for x in train_set]\n",
    "    self.test_dir = [self.data_dirs[x] for x in self.test_set]\n",
    "    self.trainskip = [skip[x] for x in train_set]\n",
    "    self.testskip = [skip[x] for x in self.test_set]\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "    \n",
    "self.train_data_file = os.path.join(self.args.save_dir, \"train_trajectories.cpkl\")\n",
    "self.test_data_file = os.path.join(self.args.save_dir, \"test_trajectories.cpkl\")\n",
    "self.train_batch_cache = os.path.join(self.args.save_dir, \"train_batch_cache.cpkl\")\n",
    "self.test_batch_cache = os.path.join(self.args.save_dir, \"test_batch_cache.cpkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. dataPreprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "setname = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "if setname == 'train':\n",
    "    data_dirs = self.train_dir\n",
    "    data_file = self.train_data_file\n",
    "else:\n",
    "    data_dirs = self.test_dir\n",
    "    data_file = self.test_data_file\n",
    "\n",
    "\n",
    "def load_dict(data_file):\n",
    "    f = open(data_file, 'rb')\n",
    "    raw_data = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    frameped_dict = raw_data[0]\n",
    "    pedtraject_dict = raw_data[1]\n",
    "\n",
    "    return frameped_dict, pedtraject_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "frameped_dict, pedtraject_dict = load_dict(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataPreprocess(self, setname):\n",
    "    '''\n",
    "    Function to load the pre-processed data into the DataLoader object\n",
    "    '''\n",
    "    if setname == 'train':\n",
    "        val_fraction = 0\n",
    "        frameped_dict = self.frameped_dict\n",
    "        pedtraject_dict = self.pedtraject_dict\n",
    "        cachefile = self.train_batch_cache\n",
    "\n",
    "    else:\n",
    "        val_fraction = 0\n",
    "        frameped_dict = self.test_frameped_dict\n",
    "        pedtraject_dict = self.test_pedtraject_dict\n",
    "        cachefile = self.test_batch_cache\n",
    "        \n",
    "    if setname != 'train':\n",
    "        shuffle = False\n",
    "    else:\n",
    "        shuffle = True\n",
    "        \n",
    "    data_index = self.get_data_index(frameped_dict, setname, ifshuffle=shuffle)\n",
    "    val_index = data_index[:, :int(data_index.shape[1] * val_fraction)]\n",
    "    train_index = data_index[:, (int(data_index.shape[1] * val_fraction) + 1):]\n",
    "    trainbatch = self.get_seq_from_index_balance(frameped_dict, pedtraject_dict, train_index, setname)\n",
    "    valbatch = self.get_seq_from_index_balance(frameped_dict, pedtraject_dict, val_index, setname)\n",
    "    trainbatchnums = len(trainbatch)\n",
    "    valbatchnums = len(valbatch)\n",
    "\n",
    "    f = open(cachefile, \"wb\")\n",
    "    pickle.dump((trainbatch, trainbatchnums, valbatch, valbatchnums), f, protocol=2)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. get_data_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_index(self, data_dict, setname, ifshuffle=True):\n",
    "    '''\n",
    "    Get the dataset sampling index.\n",
    "    data_index:\n",
    "        --> First row: frame id in set\n",
    "        --> which set\n",
    "        --> new frame id, all scenes\n",
    "    '''\n",
    "    set_id = [] # which scene is the frame of frame_id_in_set related to\n",
    "    frame_id_in_set = [] # frames in all train/test scenes\n",
    "    total_frame = 0 # total number of frames in train/test scenes\n",
    "    \n",
    "    for seti, dict in enumerate(data_dict):\n",
    "        frames = sorted(dict)\n",
    "        maxframe = max(frames) - self.args.seq_length\n",
    "        ####### TOCHECK\n",
    "        ####### Why are we subtracting 20 frames and not 20 timesteps?!\n",
    "        ####!!!!!### maxframe = max(frames) - self.args.seq_length*(frames[1] - frames[0])\n",
    "        frames = [x for x in frames if not x > maxframe]\n",
    "        total_frame += len(frames)\n",
    "        set_id.extend(list(seti for i in range(len(frames))))\n",
    "        frame_id_in_set.extend(list(frames[i] for i in range(len(frames))))\n",
    "\n",
    "    all_frame_id_list = list(i for i in range(total_frame))\n",
    "\n",
    "    data_index = np.concatenate((np.array([frame_id_in_set], dtype=int), np.array([set_id], dtype=int),\n",
    "                                 np.array([all_frame_id_list], dtype=int)), 0)\n",
    "    if ifshuffle:\n",
    "        random.Random().shuffle(all_frame_id_list)\n",
    "    data_index = data_index[:, all_frame_id_list]\n",
    "\n",
    "    # to make full use of the data. Add again at the end the fisrt #batch_size frames\n",
    "    if setname == 'train':\n",
    "        data_index = np.append(data_index, data_index[:, :self.args.batch_size], 1)\n",
    "    return data_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = get_data_index(self, frameped_dict, setname, ifshuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[780, 786, 792, ..., 810, 816, 822],\n",
       "       [  0,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   1,   2, ...,   5,   6,   7]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
