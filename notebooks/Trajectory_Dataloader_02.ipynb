{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play with Trajectory_Dataloader class #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME_TO_NUM = {\n",
    "    'eth': 0,\n",
    "    'hotel': 1,\n",
    "    'zara1': 2,\n",
    "    'zara2': 3,\n",
    "    'univ': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "args = Object()\n",
    "args.dataset = 'eth5'\n",
    "args.test_set = 'zara1'\n",
    "args.save_dir = './output/' + args.test_set + \"/\"\n",
    "args.seq_length = 20\n",
    "args.obs_length = 20\n",
    "args.neighbor_thred = 10\n",
    "args.batch_size = 8\n",
    "args.batch_around_ped = 256\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.makedirs(args.save_dir)\n",
    "\n",
    "self = Object()\n",
    "self.args = args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if self.args.dataset == 'eth5':\n",
    "\n",
    "    self.data_dirs = ['../data/eth/univ', '../data/eth/hotel',\n",
    "                      '../data/ucy/zara/zara01', '../data/ucy/zara/zara02',\n",
    "                      '../data/ucy/univ/students001', '../data/ucy/univ/students003',\n",
    "                      '../data/ucy/univ/uni_examples', '../data/ucy/zara/zara03']\n",
    "\n",
    "    # Data directory where the pre-processed pickle file resides\n",
    "    self.data_dir = './data'\n",
    "    skip = [6, 10, 10, 10, 10, 10, 10, 10]\n",
    "\n",
    "    train_set = [i for i in range(len(self.data_dirs))]\n",
    "\n",
    "    assert args.test_set in DATASET_NAME_TO_NUM.keys(), 'Unsupported dataset {}'.format(args.test_set)\n",
    "\n",
    "    args.test_set = DATASET_NAME_TO_NUM[args.test_set]\n",
    "\n",
    "    if args.test_set == 4 or args.test_set == 5:\n",
    "        self.test_set = [4, 5]\n",
    "    else:\n",
    "        self.test_set = [self.args.test_set]\n",
    "\n",
    "    for x in self.test_set:\n",
    "        train_set.remove(x)\n",
    "\n",
    "    self.train_dir = [self.data_dirs[x] for x in train_set]\n",
    "    self.test_dir = [self.data_dirs[x] for x in self.test_set]\n",
    "    self.trainskip = [skip[x] for x in train_set]\n",
    "    self.testskip = [skip[x] for x in self.test_set]\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "    \n",
    "self.train_data_file = os.path.join(self.args.save_dir, \"train_trajectories.cpkl\")\n",
    "self.test_data_file = os.path.join(self.args.save_dir, \"test_trajectories.cpkl\")\n",
    "self.train_batch_cache = os.path.join(self.args.save_dir, \"train_batch_cache.cpkl\")\n",
    "self.test_batch_cache = os.path.join(self.args.save_dir, \"test_batch_cache.cpkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. dataPreprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "setname = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if setname == 'train':\n",
    "    data_dirs = self.train_dir\n",
    "    data_file = self.train_data_file\n",
    "else:\n",
    "    data_dirs = self.test_dir\n",
    "    data_file = self.test_data_file\n",
    "\n",
    "\n",
    "def load_dict(data_file):\n",
    "    f = open(data_file, 'rb')\n",
    "    raw_data = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    frameped_dict = raw_data[0]\n",
    "    pedtraject_dict = raw_data[1]\n",
    "\n",
    "    return frameped_dict, pedtraject_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "frameped_dict, pedtraject_dict = load_dict(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataPreprocess(self, setname):\n",
    "    '''\n",
    "    Function to load the pre-processed data into the DataLoader object\n",
    "    '''\n",
    "    if setname == 'train':\n",
    "        val_fraction = 0\n",
    "        frameped_dict = self.frameped_dict\n",
    "        pedtraject_dict = self.pedtraject_dict\n",
    "        cachefile = self.train_batch_cache\n",
    "\n",
    "    else:\n",
    "        val_fraction = 0\n",
    "        frameped_dict = self.test_frameped_dict\n",
    "        pedtraject_dict = self.test_pedtraject_dict\n",
    "        cachefile = self.test_batch_cache\n",
    "        \n",
    "    if setname != 'train':\n",
    "        shuffle = False\n",
    "    else:\n",
    "        shuffle = True\n",
    "        \n",
    "    data_index = self.get_data_index(frameped_dict, setname, ifshuffle=shuffle)\n",
    "    val_index = data_index[:, :int(data_index.shape[1] * val_fraction)]\n",
    "    train_index = data_index[:, (int(data_index.shape[1] * val_fraction) + 1):]\n",
    "    trainbatch = self.get_seq_from_index_balance(frameped_dict, pedtraject_dict, train_index, setname)\n",
    "    valbatch = self.get_seq_from_index_balance(frameped_dict, pedtraject_dict, val_index, setname)\n",
    "    trainbatchnums = len(trainbatch)\n",
    "    valbatchnums = len(valbatch)\n",
    "\n",
    "    f = open(cachefile, \"wb\")\n",
    "    pickle.dump((trainbatch, trainbatchnums, valbatch, valbatchnums), f, protocol=2)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. get_data_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_index(self, data_dict, setname, ifshuffle=True):\n",
    "    '''\n",
    "    Get the dataset sampling index.\n",
    "    data_index:\n",
    "        --> First row: frame id in set\n",
    "        --> which set\n",
    "        --> new frame id, all scenes\n",
    "    '''\n",
    "    set_id = [] # which scene is the frame of frame_id_in_set related to\n",
    "    frame_id_in_set = [] # frames in all train/test scenes\n",
    "    total_frame = 0 # total number of frames in train/test scenes\n",
    "    \n",
    "    for seti, dict in enumerate(data_dict):\n",
    "        frames = sorted(dict)\n",
    "        maxframe = max(frames) - self.args.seq_length\n",
    "        ####### TOCHECK\n",
    "        ####### Why are we subtracting 20 frames and not 20 timesteps?!\n",
    "        ####!!!!!### maxframe = max(frames) - self.args.seq_length*(frames[1] - frames[0])\n",
    "        frames = [x for x in frames if not x > maxframe]\n",
    "        total_frame += len(frames)\n",
    "        set_id.extend(list(seti for i in range(len(frames))))\n",
    "        frame_id_in_set.extend(list(frames[i] for i in range(len(frames))))\n",
    "\n",
    "    all_frame_id_list = list(i for i in range(total_frame))\n",
    "\n",
    "    data_index = np.concatenate((np.array([frame_id_in_set], dtype=int), np.array([set_id], dtype=int),\n",
    "                                 np.array([all_frame_id_list], dtype=int)), 0)\n",
    "    if ifshuffle:\n",
    "        random.Random().shuffle(all_frame_id_list)\n",
    "    data_index = data_index[:, all_frame_id_list]\n",
    "\n",
    "    # to make full use of the data. Add again at the end the fisrt #batch_size frames\n",
    "    if setname == 'train':\n",
    "        data_index = np.append(data_index, data_index[:, :self.args.batch_size], 1)\n",
    "    return data_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = get_data_index(self, frameped_dict, setname, ifshuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[780, 786, 792, ..., 810, 816, 822],\n",
       "       [  0,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   1,   2, ...,   5,   6,   7]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. get_seq_from_index_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 6102\n",
      "100 / 6102\n",
      "200 / 6102\n",
      "300 / 6102\n",
      "400 / 6102\n",
      "500 / 6102\n",
      "600 / 6102\n",
      "700 / 6102\n",
      "800 / 6102\n",
      "900 / 6102\n",
      "1000 / 6102\n",
      "1100 / 6102\n",
      "1200 / 6102\n",
      "1300 / 6102\n",
      "1400 / 6102\n",
      "1500 / 6102\n",
      "1600 / 6102\n",
      "1700 / 6102\n",
      "1800 / 6102\n",
      "1900 / 6102\n",
      "2000 / 6102\n",
      "2100 / 6102\n",
      "2200 / 6102\n",
      "2300 / 6102\n",
      "2400 / 6102\n",
      "2500 / 6102\n",
      "2600 / 6102\n",
      "2700 / 6102\n",
      "2800 / 6102\n",
      "2900 / 6102\n",
      "3000 / 6102\n",
      "3100 / 6102\n",
      "3200 / 6102\n",
      "3300 / 6102\n",
      "3400 / 6102\n",
      "3500 / 6102\n",
      "3600 / 6102\n",
      "3700 / 6102\n",
      "3800 / 6102\n",
      "3900 / 6102\n",
      "4000 / 6102\n",
      "4100 / 6102\n",
      "4200 / 6102\n",
      "4300 / 6102\n",
      "4400 / 6102\n",
      "4500 / 6102\n",
      "4600 / 6102\n",
      "4700 / 6102\n",
      "4800 / 6102\n",
      "4900 / 6102\n",
      "5000 / 6102\n",
      "5100 / 6102\n",
      "5200 / 6102\n",
      "5300 / 6102\n",
      "5400 / 6102\n",
      "5500 / 6102\n",
      "5600 / 6102\n",
      "5700 / 6102\n",
      "5800 / 6102\n",
      "5900 / 6102\n",
      "6000 / 6102\n",
      "6100 / 6102\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Query the trajectories fragments from data sampling index.\n",
    "Notes: Divide the scene if there are too many people; accumulate the scene if there are few people.\n",
    "       This function takes less gpu memory.\n",
    "'''\n",
    "batch_data_mass = []\n",
    "batch_data = []\n",
    "Batch_id = []\n",
    "\n",
    "temp = self.args.batch_around_ped\n",
    "if setname == 'train':\n",
    "    skip = self.trainskip\n",
    "else:\n",
    "    skip = self.testskip\n",
    "\n",
    "ped_cnt = 0\n",
    "last_frame = 0\n",
    "for i in range(data_index.shape[1]):\n",
    "    if i % 100 == 0:\n",
    "        print(i, '/', data_index.shape[1])\n",
    "    cur_frame, cur_set, _ = data_index[:, i]\n",
    "    framestart_pedi = set(frameped_dict[cur_set][cur_frame])\n",
    "    try:\n",
    "        frameend_pedi = set(frameped_dict[cur_set][cur_frame + self.args.seq_length * skip[cur_set]])\n",
    "    except:\n",
    "        continue\n",
    "    present_pedi = framestart_pedi | frameend_pedi\n",
    "    if (framestart_pedi & frameend_pedi).__len__() == 0:\n",
    "        continue\n",
    "    traject = ()\n",
    "    IFfull = []\n",
    "    for ped in present_pedi:\n",
    "        cur_trajec, iffull, ifexistobs = find_trajectory_fragment(pedtraject_dict[cur_set][ped], cur_frame,\n",
    "                                                                       self.args.seq_length, skip[cur_set])\n",
    "        if len(cur_trajec) == 0:\n",
    "            continue\n",
    "        if ifexistobs == False:\n",
    "            # Just ignore trajectories if their data don't exsist at the last obversed time step (easy for data shift)\n",
    "            continue\n",
    "        if sum(cur_trajec[:, 0] > 0) < 5:\n",
    "            # filter trajectories have too few frame data\n",
    "            continue\n",
    "\n",
    "        cur_trajec = (cur_trajec[:, 1:].reshape(-1, 1, 2),)\n",
    "        traject = traject.__add__(cur_trajec)\n",
    "        IFfull.append(iffull)\n",
    "    if traject.__len__() < 1:\n",
    "        continue\n",
    "    if sum(IFfull) < 1:\n",
    "        continue\n",
    "    traject_batch = np.concatenate(traject, 1)\n",
    "    batch_pednum = sum([i.shape[1] for i in batch_data]) + traject_batch.shape[1]\n",
    "\n",
    "    cur_pednum = traject_batch.shape[1]\n",
    "    ped_cnt += cur_pednum\n",
    "    batch_id = (cur_set, cur_frame,)\n",
    "\n",
    "    if cur_pednum >= self.args.batch_around_ped * 2:\n",
    "        # too many people in current scene\n",
    "        # split the scene into two batches\n",
    "        ind = traject_batch[self.args.obs_length - 1].argsort(0)\n",
    "        cur_batch_data, cur_Batch_id = [], []\n",
    "        Seq_batchs = [traject_batch[:, ind[:cur_pednum // 2, 0]], traject_batch[:, ind[cur_pednum // 2:, 0]]]\n",
    "        for sb in Seq_batchs:\n",
    "            cur_batch_data.append(sb)\n",
    "            cur_Batch_id.append(batch_id)\n",
    "            cur_batch_data = self.massup_batch(cur_batch_data)\n",
    "            batch_data_mass.append((cur_batch_data, cur_Batch_id,))\n",
    "            cur_batch_data = []\n",
    "            cur_Batch_id = []\n",
    "\n",
    "        last_frame = i\n",
    "    elif cur_pednum >= self.args.batch_around_ped:\n",
    "        # good pedestrian numbers\n",
    "        cur_batch_data, cur_Batch_id = [], []\n",
    "        cur_batch_data.append(traject_batch)\n",
    "        cur_Batch_id.append(batch_id)\n",
    "        cur_batch_data = self.massup_batch(cur_batch_data)\n",
    "        batch_data_mass.append((cur_batch_data, cur_Batch_id,))\n",
    "\n",
    "        last_frame = i\n",
    "    else:  # less pedestrian numbers <64\n",
    "        # accumulate multiple framedata into a batch\n",
    "        if batch_pednum > self.args.batch_around_ped:\n",
    "            # enough people in the scene\n",
    "            batch_data.append(traject_batch)\n",
    "            Batch_id.append(batch_id)\n",
    "\n",
    "            batch_data = massup_batch(batch_data)\n",
    "            batch_data_mass.append((batch_data, Batch_id,))\n",
    "\n",
    "            last_frame = i\n",
    "            batch_data = []\n",
    "            Batch_id = []\n",
    "        else:\n",
    "            batch_data.append(traject_batch)\n",
    "            Batch_id.append(batch_id)\n",
    "\n",
    "if last_frame < data_index.shape[1] - 1 and setname == 'test' and batch_pednum > 1:\n",
    "    batch_data = self.massup_batch(batch_data)\n",
    "    batch_data_mass.append((batch_data, Batch_id,))\n",
    "self.args.batch_around_ped = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_trajectory_fragment(trajectory, startframe, seq_length, skip):\n",
    "    '''\n",
    "    Query the trajectory fragment based on the index. Replace where data isn't exsist with 0.\n",
    "    '''\n",
    "    return_trajec = np.zeros((seq_length, 3))\n",
    "    endframe = startframe + (seq_length) * skip\n",
    "    start_n = np.where(trajectory[:, 0] == startframe)\n",
    "    end_n = np.where(trajectory[:, 0] == endframe)\n",
    "    iffull = False\n",
    "    ifexsitobs = False\n",
    "\n",
    "    if start_n[0].shape[0] == 0 and end_n[0].shape[0] != 0:\n",
    "        start_n = 0\n",
    "        end_n = end_n[0][0]\n",
    "        if end_n == 0:\n",
    "            return return_trajec, iffull, ifexsitobs\n",
    "\n",
    "    elif end_n[0].shape[0] == 0 and start_n[0].shape[0] != 0:\n",
    "        start_n = start_n[0][0]\n",
    "        end_n = trajectory.shape[0]\n",
    "\n",
    "    elif end_n[0].shape[0] == 0 and start_n[0].shape[0] == 0:\n",
    "        start_n = 0\n",
    "        end_n = trajectory.shape[0]\n",
    "\n",
    "    else:\n",
    "        end_n = end_n[0][0]\n",
    "        start_n = start_n[0][0]\n",
    "\n",
    "    candidate_seq = trajectory[start_n:end_n]\n",
    "    offset_start = int((candidate_seq[0, 0] - startframe) // skip)\n",
    "\n",
    "    offset_end = self.args.seq_length + int((candidate_seq[-1, 0] - endframe) // skip)\n",
    "\n",
    "    return_trajec[offset_start:offset_end + 1, :3] = candidate_seq\n",
    "\n",
    "    if return_trajec[self.args.obs_length - 1, 1] != 0:\n",
    "        ifexsitobs = True\n",
    "\n",
    "    if offset_end - offset_start >= seq_length - 1:\n",
    "        iffull = True\n",
    "\n",
    "    return return_trajec, iffull, ifexsitobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def massup_batch(batch_data):\n",
    "    '''\n",
    "    Massed up data fragements in different time window together to a batch\n",
    "    '''\n",
    "    num_Peds = 0\n",
    "    for batch in batch_data:\n",
    "        num_Peds += batch.shape[1]\n",
    "\n",
    "    seq_list_b = np.zeros((self.args.seq_length, 0))\n",
    "    nodes_batch_b = np.zeros((self.args.seq_length, 0, 2))\n",
    "    nei_list_b = np.zeros((self.args.seq_length, num_Peds, num_Peds))\n",
    "    nei_num_b = np.zeros((self.args.seq_length, num_Peds))\n",
    "    num_Ped_h = 0\n",
    "    batch_pednum = []\n",
    "    for batch in batch_data:\n",
    "        num_Ped = batch.shape[1]\n",
    "        seq_list, nei_list, nei_num = get_social_inputs_numpy(batch)\n",
    "        nodes_batch_b = np.append(nodes_batch_b, batch, 1)\n",
    "        seq_list_b = np.append(seq_list_b, seq_list, 1)\n",
    "        nei_list_b[:, num_Ped_h:num_Ped_h + num_Ped, num_Ped_h:num_Ped_h + num_Ped] = nei_list\n",
    "        nei_num_b[:, num_Ped_h:num_Ped_h + num_Ped] = nei_num\n",
    "        batch_pednum.append(num_Ped)\n",
    "        num_Ped_h += num_Ped\n",
    "    return (nodes_batch_b, seq_list_b, nei_list_b, nei_num_b, batch_pednum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_social_inputs_numpy(inputnodes):\n",
    "    '''\n",
    "    Get the sequence list (denoting where data exsist) and neighboring list (denoting where neighbors exsist).\n",
    "    '''\n",
    "    num_Peds = inputnodes.shape[1]\n",
    "\n",
    "    seq_list = np.zeros((inputnodes.shape[0], num_Peds))\n",
    "    # denote where data not missing\n",
    "\n",
    "    for pedi in range(num_Peds):\n",
    "        seq = inputnodes[:, pedi]\n",
    "        seq_list[seq[:, 0] != 0, pedi] = 1\n",
    "\n",
    "    # get relative cords, neighbor id list\n",
    "    nei_list = np.zeros((inputnodes.shape[0], num_Peds, num_Peds))\n",
    "    nei_num = np.zeros((inputnodes.shape[0], num_Peds))\n",
    "\n",
    "    # nei_list[f,i,j] denote if j is i's neighbors in frame f\n",
    "    for pedi in range(num_Peds):\n",
    "        nei_list[:, pedi, :] = seq_list\n",
    "        nei_list[:, pedi, pedi] = 0  # person i is not the neighbor of itself\n",
    "        nei_num[:, pedi] = np.sum(nei_list[:, pedi, :], 1)\n",
    "        seqi = inputnodes[:, pedi]\n",
    "        for pedj in range(num_Peds):\n",
    "            seqj = inputnodes[:, pedj]\n",
    "            select = (seq_list[:, pedi] > 0) & (seq_list[:, pedj] > 0)\n",
    "\n",
    "            relative_cord = seqi[select, :2] - seqj[select, :2]\n",
    "\n",
    "            # invalid data index\n",
    "            select_dist = (abs(relative_cord[:, 0]) > self.args.neighbor_thred) | (\n",
    "                    abs(relative_cord[:, 1]) > self.args.neighbor_thred)\n",
    "\n",
    "            nei_num[select, pedi] -= select_dist\n",
    "\n",
    "            select[select == True] = select_dist\n",
    "            nei_list[select, pedi, pedj] = 0\n",
    "    return seq_list, nei_list, nei_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
